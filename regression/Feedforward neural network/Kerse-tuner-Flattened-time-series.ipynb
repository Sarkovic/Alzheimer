{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline \n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"all_data_LSTM_full_feature.csv\")\n",
    "test=pd.read_csv(\"y_DX_month.csv\")\n",
    "df=df.sort_values(by='RID', ascending=True)\n",
    "test=test.sort_values(by='RID', ascending=True)\n",
    "groupby=df.groupby(\"RID\").count()\n",
    "len(groupby)\n",
    "df=df.drop(['RID', 'VISCODE2'],axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(df.columns)\n",
    "y= np.array(test.DX_Month)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(df)\n",
    "X = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "y = scaler2.fit_transform(pd.DataFrame(y))\n",
    "from sklearn. model_selection import train_test_split\n",
    "x= X.reshape( 613, 4* num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     x, y, test_size=0.10, stratify= y, shuffle=True, random_state=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    num_units_min  =  400\n",
    "    num_units_max  =  1000\n",
    "    num_units_step =  50\n",
    "\n",
    "    dropout_min  =  .2\n",
    "    dropout_max  =  0.5\n",
    "    dropout_step =  0.1\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('unit1',  min_value=num_units_min,\n",
    "                                                 max_value=num_units_max,\n",
    "                                                 step=num_units_step),\n",
    "                                                 activation='relu',\n",
    "                                                 input_dim=4* num_features, \n",
    "                                                 kernel_initializer='normal', \n",
    "                                                 kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate',values=[0.01, 0.05, 0.1]))\n",
    "                          \n",
    "                                               ))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1',min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step) ) )\n",
    " \n",
    " \n",
    "    model.add(layers.Flatten())\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "        model.add(layers.Dense(units=hp.Int('unitsdense_'+ str(i),\n",
    "                                            min_value=200,\n",
    "                                            max_value=800,\n",
    "                                            step=200),\n",
    "                                            activation='relu',\n",
    "                              kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate1',values=[0.01, 0.05, 0.1]))))   \n",
    "        model.add(layers.Dropout(hp.Float('dropoutdense_'+ str(i), \n",
    "                                      min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',values=[ 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\hager5\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\hager5\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='mean_squared_error',\n",
    "    max_trials=4,\n",
    "  \n",
    "    project_name='hager5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000002BFDFC76FD0> and <tensorflow.python.keras.layers.core.Dropout object at 0x000002BFE1FD0970>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000002BFE1F88340> and <tensorflow.python.keras.layers.core.Dropout object at 0x000002BFE0F95B80>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000002BFE1F888E0> and <tensorflow.python.keras.layers.core.Dropout object at 0x000002BFE0F20430>).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor='mean_squared_error', patience=30, verbose=1)\n",
    "callback_list = [ early_stopping ]\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=60,\n",
    "             batch_size=50, \n",
    "             callbacks=callback_list,\n",
    "             validation_data=(X_test,y_test))\n",
    "model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 650)               343850    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 650)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 650)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               260400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 765,051\n",
      "Trainable params: 765,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unit1': 650,\n",
       " 'reg_rate': 0.01,\n",
       " 'dropout_1': 0.2,\n",
       " 'num_layers': 2,\n",
       " 'unitsdense_0': 400,\n",
       " 'reg_rate1': 0.05,\n",
       " 'dropoutdense_0': 0.4000000000000001,\n",
       " 'learning_rate': 0.0001,\n",
       " 'unitsdense_1': 400,\n",
       " 'dropoutdense_1': 0.5000000000000001,\n",
       " 'unitsdense_2': 200,\n",
       " 'dropoutdense_2': 0.30000000000000004,\n",
       " 'unitsdense_3': 200,\n",
       " 'dropoutdense_3': 0.30000000000000004}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Model\")\n",
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000002BFE0EF2280> and <tensorflow.python.keras.layers.core.Dropout object at 0x000002BFE1F88F40>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000002BFE1FD05E0> and <tensorflow.python.keras.layers.core.Dropout object at 0x000002BFE044BF70>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000002BFE2043AF0> and <tensorflow.python.keras.layers.core.Dropout object at 0x000002BFE2049400>).\n",
      "2/2 [==============================] - 1s 6ms/step - loss: 5.5614 - mean_squared_error: 0.0473 - mean_absolute_error: 0.1366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05947320908308029"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict_Train={\"RMSE\": [],\"MSE\": [],\"MBE\": []};\n",
    "my_dict_Test={\"RMSE\": [],\"MSE\": [],\"MBE\": []};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0663 - mean_squared_error: 0.0649 - mean_absolute_error: 0.1743 - val_loss: 0.0655 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1721\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0664 - mean_squared_error: 0.0650 - mean_absolute_error: 0.1736 - val_loss: 0.0654 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1714\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0664 - mean_squared_error: 0.0651 - mean_absolute_error: 0.1733 - val_loss: 0.0653 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1705\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0664 - mean_squared_error: 0.0652 - mean_absolute_error: 0.1729 - val_loss: 0.0653 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1702\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0660 - mean_squared_error: 0.0649 - mean_absolute_error: 0.1722 - val_loss: 0.0652 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1703\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0661 - mean_squared_error: 0.0650 - mean_absolute_error: 0.1725 - val_loss: 0.0652 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1700\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0664 - mean_squared_error: 0.0653 - mean_absolute_error: 0.1720 - val_loss: 0.0651 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1693\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0661 - mean_squared_error: 0.0651 - mean_absolute_error: 0.1715 - val_loss: 0.0651 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1689\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0657 - mean_squared_error: 0.0647 - mean_absolute_error: 0.1706 - val_loss: 0.0651 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1687\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0661 - mean_squared_error: 0.0652 - mean_absolute_error: 0.1702 - val_loss: 0.0650 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1686\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0664 - mean_squared_error: 0.0655 - mean_absolute_error: 0.1713 - val_loss: 0.0650 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1685\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0657 - mean_squared_error: 0.0648 - mean_absolute_error: 0.1707 - val_loss: 0.0649 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1694\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0659 - mean_squared_error: 0.0651 - mean_absolute_error: 0.1716 - val_loss: 0.0649 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1693\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0662 - mean_squared_error: 0.0654 - mean_absolute_error: 0.1713 - val_loss: 0.0649 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1685\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0655 - mean_squared_error: 0.0647 - mean_absolute_error: 0.1698 - val_loss: 0.0648 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1683\n",
      "Epoch 1/15\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0661 - mean_squared_error: 0.0654 - mean_absolute_error: 0.1706 - val_loss: 0.0648 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1678\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0656 - mean_squared_error: 0.0649 - mean_absolute_error: 0.1700 - val_loss: 0.0648 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1676\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0659 - mean_squared_error: 0.0653 - mean_absolute_error: 0.1701 - val_loss: 0.0648 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1672\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0659 - mean_squared_error: 0.0653 - mean_absolute_error: 0.1695 - val_loss: 0.0647 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1671\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0661 - mean_squared_error: 0.0655 - mean_absolute_error: 0.1700 - val_loss: 0.0647 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1668\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0659 - mean_squared_error: 0.0653 - mean_absolute_error: 0.1688 - val_loss: 0.0647 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1663\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0659 - mean_squared_error: 0.0654 - mean_absolute_error: 0.1683 - val_loss: 0.0647 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1666\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0657 - mean_squared_error: 0.0651 - mean_absolute_error: 0.1707 - val_loss: 0.0646 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1696\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0656 - mean_squared_error: 0.0651 - mean_absolute_error: 0.1716 - val_loss: 0.0646 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1694\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0655 - mean_squared_error: 0.0651 - mean_absolute_error: 0.1715 - val_loss: 0.0646 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1695\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0658 - mean_squared_error: 0.0654 - mean_absolute_error: 0.1715 - val_loss: 0.0646 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1692\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0656 - mean_squared_error: 0.0652 - mean_absolute_error: 0.1725 - val_loss: 0.0645 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1712\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0654 - mean_squared_error: 0.0650 - mean_absolute_error: 0.1735 - val_loss: 0.0645 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1713\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0653 - mean_squared_error: 0.0649 - mean_absolute_error: 0.1730 - val_loss: 0.0645 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1709\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0654 - mean_squared_error: 0.0650 - mean_absolute_error: 0.1724 - val_loss: 0.0645 - val_mean_squared_error: 0.0641 - val_mean_absolute_error: 0.1700\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.10, stratify= y, shuffle=True, random_state=None)\n",
    "        history= best_model.fit(X_train, y_train, epochs=15, batch_size=50,  verbose=1,shuffle=True, validation_data=(X_test,y_test))\n",
    "        my_dict_Train[\"RMSE\"].append(np.sqrt(np.mean(history.history['val_mean_squared_error'])) )\n",
    "        my_dict_Train[\"MSE\"].append(np.mean(history.history['val_mean_squared_error']))\n",
    "        my_dict_Train[\"MBE\"].append(np.mean(history.history['val_mean_absolute_error']))\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        my_dict_Test[\"RMSE\"].append(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "        my_dict_Test[\"MSE\"].append(metrics.mean_squared_error(y_test,y_pred))\n",
    "        my_dict_Test[\"MBE\"].append(metrics.mean_absolute_error(y_test,y_pred))\n",
    "        best_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "RMSE: 0.253\n",
      "MSE: 0.064\n",
      "MBE: 0.170\n"
     ]
    }
   ],
   "source": [
    "print(\"Training result\")\n",
    "for metric in my_dict_Train.keys():\n",
    "    print(\"%s: %.3f\" % (metric, np.mean(my_dict_Train[metric])))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing result\n",
      "RMSE: 0.253\n",
      "MSE: 0.064\n",
      "MBE: 0.169\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing result\")\n",
    "for metric in my_dict_Test.keys():\n",
    "    print(\"%s: %.3f\" % (metric, np.mean(my_dict_Test[metric])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
