{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline \n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn. model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= pd.read_csv(\"all_data_LSTM_full_feature.csv\")\n",
    "test=pd.read_csv(\"y_DX_month.csv\")\n",
    "df=df.sort_values(by='RID', ascending=True)\n",
    "test=test.sort_values(by='RID', ascending=True)\n",
    "groupby=df.groupby(\"RID\").count()\n",
    "len(groupby)\n",
    "df=df.drop(['RID', 'VISCODE2'],axis = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(df.columns)\n",
    "y= np.array(test.DX_Month)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(df)\n",
    "X = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "y = scaler2.fit_transform(pd.DataFrame(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= X.reshape(613, 4, num_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     x, y, test_size=0.10, stratify= y, shuffle=True, random_state=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    num_units_min  =  300\n",
    "    num_units_max  =  700\n",
    "    num_units_step =  20\n",
    "\n",
    "    dropout_min  =  .2\n",
    "    dropout_max  =  0.5\n",
    "    dropout_step =  0.1\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(units=hp.Int('unit1',  min_value=num_units_min,\n",
    "                                                 max_value=num_units_max,\n",
    "                                                 step=num_units_step),\n",
    "                                                 activation='relu',\n",
    "                                                 input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                                                 kernel_initializer='normal', \n",
    "                                                 kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate',values=[0.01, 0.05, 0.1])),\n",
    "                          \n",
    "                                                 return_sequences = True))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1',min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step) ) )\n",
    " \n",
    "  \n",
    "    model.add(layers.Flatten())\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "        model.add(layers.Dense(units=hp.Int('unitsdense_'+ str(i),\n",
    "                                            min_value=50,\n",
    "                                            max_value=100,\n",
    "                                            step=10),\n",
    "                                            activation='relu',\n",
    "                              kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate1'+ str(i),values=[0.01, 0.05, 0.1]))))   \n",
    "        model.add(layers.Dropout(hp.Float('dropoutdense_'+ str(i), \n",
    "                                      min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',values=[ 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\hager4\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\hager4\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='mean_squared_error',\n",
    "    max_trials=4,\n",
    "  \n",
    "    project_name='hager4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.recurrent_v2.LSTM object at 0x0000012512469820> and <tensorflow.python.keras.layers.core.Dropout object at 0x000001251823E6A0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x00000125181C1D60> and <tensorflow.python.keras.layers.core.Dropout object at 0x00000125181E5850>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x00000125181ECDF0> and <tensorflow.python.keras.layers.core.Dropout object at 0x00000125181ECAC0>).\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='mean_squared_error', patience=30, verbose=1)\n",
    "callback_list = [ early_stopping ]\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=60,\n",
    "             batch_size=50, \n",
    "             callbacks=callback_list,\n",
    "             validation_data=(X_test,y_test))\n",
    "model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 4, 440)            1008480   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 440)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1760)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 60)                105660    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                4880      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 1,119,101\n",
      "Trainable params: 1,119,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unit1': 440,\n",
       " 'reg_rate': 0.1,\n",
       " 'dropout_1': 0.2,\n",
       " 'num_layers': 2,\n",
       " 'unitsdense_0': 60,\n",
       " 'reg_rate1': 0.01,\n",
       " 'dropoutdense_0': 0.30000000000000004,\n",
       " 'learning_rate': 0.0001,\n",
       " 'unitsdense_1': 80,\n",
       " 'dropoutdense_1': 0.30000000000000004,\n",
       " 'unitsdense_2': 70,\n",
       " 'dropoutdense_2': 0.2,\n",
       " 'reg_rate10': 0.01,\n",
       " 'reg_rate11': 0.01}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.recurrent_v2.LSTM object at 0x0000012515E44520> and <tensorflow.python.keras.layers.core.Dropout object at 0x0000012518206940>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x00000125183E46D0> and <tensorflow.python.keras.layers.core.Dropout object at 0x00000125183E86A0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x00000125183F3B80> and <tensorflow.python.keras.layers.core.Dropout object at 0x00000125183F3F40>).\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict_Train={\"RMSE\": [],\"MSE\": [],\"MBE\": []};\n",
    "my_dict_Test={\"RMSE\": [],\"MSE\": [],\"MBE\": []};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 2.5150 - mean_squared_error: 0.0591 - mean_absolute_error: 0.1520 - val_loss: 2.4602 - val_mean_squared_error: 0.0561 - val_mean_absolute_error: 0.1580\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 2.4273 - mean_squared_error: 0.0595 - mean_absolute_error: 0.1583 - val_loss: 2.3739 - val_mean_squared_error: 0.0560 - val_mean_absolute_error: 0.1553\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 2.3414 - mean_squared_error: 0.0582 - mean_absolute_error: 0.1678 - val_loss: 2.2919 - val_mean_squared_error: 0.0566 - val_mean_absolute_error: 0.1634\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 2.2603 - mean_squared_error: 0.0587 - mean_absolute_error: 0.1485 - val_loss: 2.2115 - val_mean_squared_error: 0.0563 - val_mean_absolute_error: 0.1453\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 2.1825 - mean_squared_error: 0.0600 - mean_absolute_error: 0.1484 - val_loss: 2.1342 - val_mean_squared_error: 0.0566 - val_mean_absolute_error: 0.1548\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 2.1059 - mean_squared_error: 0.0597 - mean_absolute_error: 0.1578 - val_loss: 2.0594 - val_mean_squared_error: 0.0564 - val_mean_absolute_error: 0.1455\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 2.0312 - mean_squared_error: 0.0585 - mean_absolute_error: 0.1439 - val_loss: 1.9887 - val_mean_squared_error: 0.0577 - val_mean_absolute_error: 0.1693\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 1.9688 - mean_squared_error: 0.0669 - mean_absolute_error: 0.2081 - val_loss: 1.9240 - val_mean_squared_error: 0.0625 - val_mean_absolute_error: 0.1920\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 1.8940 - mean_squared_error: 0.0608 - mean_absolute_error: 0.1648 - val_loss: 1.8543 - val_mean_squared_error: 0.0599 - val_mean_absolute_error: 0.1411\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 1.8292 - mean_squared_error: 0.0621 - mean_absolute_error: 0.1402 - val_loss: 1.7896 - val_mean_squared_error: 0.0600 - val_mean_absolute_error: 0.1428\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 1.7639 - mean_squared_error: 0.0606 - mean_absolute_error: 0.1468 - val_loss: 1.7261 - val_mean_squared_error: 0.0589 - val_mean_absolute_error: 0.1553\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 1.7027 - mean_squared_error: 0.0608 - mean_absolute_error: 0.1577 - val_loss: 1.6657 - val_mean_squared_error: 0.0586 - val_mean_absolute_error: 0.1571\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 1.6420 - mean_squared_error: 0.0593 - mean_absolute_error: 0.1545 - val_loss: 1.6073 - val_mean_squared_error: 0.0580 - val_mean_absolute_error: 0.1511\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 1.5862 - mean_squared_error: 0.0605 - mean_absolute_error: 0.1444 - val_loss: 1.5514 - val_mean_squared_error: 0.0579 - val_mean_absolute_error: 0.1477\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 1.5298 - mean_squared_error: 0.0591 - mean_absolute_error: 0.1518 - val_loss: 1.4973 - val_mean_squared_error: 0.0577 - val_mean_absolute_error: 0.1556\n",
      "Epoch 1/15\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 1.4755 - mean_squared_error: 0.0577 - mean_absolute_error: 0.1530 - val_loss: 1.4470 - val_mean_squared_error: 0.0592 - val_mean_absolute_error: 0.1525\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 1.4258 - mean_squared_error: 0.0591 - mean_absolute_error: 0.1484 - val_loss: 1.3975 - val_mean_squared_error: 0.0596 - val_mean_absolute_error: 0.1640\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 1.3804 - mean_squared_error: 0.0626 - mean_absolute_error: 0.1833 - val_loss: 1.3509 - val_mean_squared_error: 0.0609 - val_mean_absolute_error: 0.1754\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 1.3308 - mean_squared_error: 0.0602 - mean_absolute_error: 0.1745 - val_loss: 1.3031 - val_mean_squared_error: 0.0594 - val_mean_absolute_error: 0.1512\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.2841 - mean_squared_error: 0.0592 - mean_absolute_error: 0.1464 - val_loss: 1.2583 - val_mean_squared_error: 0.0595 - val_mean_absolute_error: 0.1471\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 1.2389 - mean_squared_error: 0.0583 - mean_absolute_error: 0.1435 - val_loss: 1.2151 - val_mean_squared_error: 0.0597 - val_mean_absolute_error: 0.1624\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 1.2006 - mean_squared_error: 0.0627 - mean_absolute_error: 0.1862 - val_loss: 1.1736 - val_mean_squared_error: 0.0599 - val_mean_absolute_error: 0.1651\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 1.1555 - mean_squared_error: 0.0586 - mean_absolute_error: 0.1452 - val_loss: 1.1339 - val_mean_squared_error: 0.0605 - val_mean_absolute_error: 0.1399\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 1.1151 - mean_squared_error: 0.0579 - mean_absolute_error: 0.1468 - val_loss: 1.0948 - val_mean_squared_error: 0.0601 - val_mean_absolute_error: 0.1687\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 1.0794 - mean_squared_error: 0.0603 - mean_absolute_error: 0.1746 - val_loss: 1.0567 - val_mean_squared_error: 0.0592 - val_mean_absolute_error: 0.1522\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 1.0404 - mean_squared_error: 0.0580 - mean_absolute_error: 0.1390 - val_loss: 1.0208 - val_mean_squared_error: 0.0593 - val_mean_absolute_error: 0.1491\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 1.0052 - mean_squared_error: 0.0583 - mean_absolute_error: 0.1701 - val_loss: 0.9861 - val_mean_squared_error: 0.0593 - val_mean_absolute_error: 0.1629\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.9724 - mean_squared_error: 0.0599 - mean_absolute_error: 0.1408 - val_loss: 0.9532 - val_mean_squared_error: 0.0604 - val_mean_absolute_error: 0.1401\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.9378 - mean_squared_error: 0.0587 - mean_absolute_error: 0.1481 - val_loss: 0.9199 - val_mean_squared_error: 0.0597 - val_mean_absolute_error: 0.1626\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.9066 - mean_squared_error: 0.0595 - mean_absolute_error: 0.1626 - val_loss: 0.8886 - val_mean_squared_error: 0.0596 - val_mean_absolute_error: 0.1500\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.10, stratify= y, shuffle=True, random_state=None)\n",
    "        history= best_model.fit(X_train, y_train, epochs=15, batch_size=50,  verbose=1,shuffle=True, validation_data=(X_test,y_test))\n",
    "        my_dict_Train[\"RMSE\"].append(np.sqrt(np.mean(history.history['val_mean_squared_error'])) )\n",
    "        my_dict_Train[\"MSE\"].append(np.mean(history.history['val_mean_squared_error']))\n",
    "        my_dict_Train[\"MBE\"].append(np.mean(history.history['val_mean_absolute_error']))\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        my_dict_Test[\"RMSE\"].append(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "        my_dict_Test[\"MSE\"].append(metrics.mean_squared_error(y_test,y_pred))\n",
    "        my_dict_Test[\"MBE\"].append(metrics.mean_absolute_error(y_test,y_pred))\n",
    "        best_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "RMSE: 0.243\n",
      "MSE: 0.059\n",
      "MBE: 0.152\n"
     ]
    }
   ],
   "source": [
    "print(\"Training result\")\n",
    "for metric in my_dict_Train.keys():\n",
    "    print(\"%s: %.3f\" % (metric, np.mean(my_dict_Train[metric])))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing result\n",
      "RMSE: 0.242\n",
      "MSE: 0.059\n",
      "MBE: 0.153\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing result\")\n",
    "for metric in my_dict_Test.keys():\n",
    "    print(\"%s: %.3f\" % (metric, np.mean(my_dict_Test[metric])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
